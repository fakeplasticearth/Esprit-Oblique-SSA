\documentclass[specialist, substylefile = spbureport.rtx, subf,href,colorlinks=true, 12pt]{disser}

\usepackage[a4paper,
            mag=1000, includefoot,
            left=3cm, right=1.5cm, top=2cm, bottom=2cm, headsep=1cm, footskip=1cm]{geometry}
\usepackage{mathtext}
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{float}
\usepackage{bbold}
\usepackage{dsfont}
\usepackage{graphicx}
\usepackage{ textcomp }
\usepackage{subcaption}
\usepackage{mathdots}
\usepackage{multirow}
\usepackage{longtable,booktabs,array}
\ifpdf\usepackage{epstopdf}\fi
\bibliographystyle{gost2008}

% Точка с запятой в качестве разделителя между номерами цитирований
%\setcitestyle{semicolon}

% Использовать полужирное начертание для векторов
\let\vec=\mathbf

% Включать подсекции в оглавление
\setcounter{tocdepth}{2}

\graphicspath{{fig/}}

\theoremstyle{definition}
\newtheorem{definition}{Определение}
\newtheorem{algorithm}{Алгоритм}
\newtheorem{remark}{Замечание}
\newtheorem{theorem}{Теорема}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\diag}{diag}

%----------------------------------------------------------------


%-------------------------------
\begin{document}

%
% Титульный лист на русском языке
%
% Название организации
\institution{%
    Санкт-Петербургский государственный университет\\
    Прикладная математика и информатика
}

\title{Отчет по научно-исследовательской работе}

% Тема
\topic{Улучшение разделимости и автоматизация метода SSA для анализа временных рядов}

% Автор
\author{Дудник Павел Дмитриевич}
\group{группа 19.Б04-мм}

% Научный руководитель
\sa       {Голяндина Нина Эдуардовна\\%
           Кафедра Статистического Моделирования}
\sastatus {к.\,ф.-м.\,н., доцент}

% Город и год
\city{Санкт-Петербург}
\date{\number\year}

\chapter{Вспомогательные результаты}
\label{ch:1}
\section{Модель временного ряда}
\begin{definition}
    ~$F_N = (f_1, \ldots, f_N)$ --- \textbf{временной ряд} длины $N$, $f_i \in \mathbb{R}$ --- наблюдение в момент времени $i$.

    $F_N = F_{Signal} + F_{Noise}$, $F_{Signal} = F_{Trend} + F_{Periodics}$ --- детерминированная составляющая; $F_{Trend}, F_{Periodics}, F_{Noise}$ --- временные ряды длины N, компоненты ряда $F$.
    \begin{itemize}
        \item $F_{Trend}$ --- тренд, медленно меняющаяся компонента
        \item $F_{Periodics}$ --- остаток, сумма периодических компонент
        \item $F_{Noise}$ --- шум, случайная составляющая
    \end{itemize}
\end{definition}
\section{Метод <<Гусеница>>-SSA}
\label{sec12}
\begin{algorithm}
\label{alg1}
    Входные данные: $F_N=(f_1, \ldots, f_N)$ --- временной ряд, $1<L<N$ --- длина окна, $r$ --- число компонент, группировка компонент (разбиение множества индексов).\\
    Результат: разложение ряда $F$ в сумму компонент: $F_N = \sum_{i = 1}^{r}\widetilde{\mathbb{X}}_i$.
    \begin{enumerate}
	    \item \textbf{Вложение}: $K=N-L+1$, ряд переводится в траекторную матрицу $\bm{X}=[X_1:\ldots:X_K]$, где $X_i = (f_i, \ldots, f_{i + L - 1})^{\mathrm{T}}$, $1 \leq i \leq K$.

	    \item Строится SVD (сингулярное разложение) траекторной матрицы: $\bm{X} = \sum\limits_{i=1}^d \sqrt{\lambda_i}U_iV_i^{\mathrm{T}} = \sum\limits_{i=1}^d\bm{X}_i$, $1 \leq i \leq d = \rank\bm{X}$, $\sqrt{\lambda_i}$ --- сингулярные числа, $\bm{X}_i$ --- траекторная матрица элементарной компоненты (временного ряда, который соответствует одному слагаемому в сингулярном разложении).

	    \item \textbf{Группировка} \label{it:3}: $\bm{X} =  \sum\limits_{i=1}^r \widetilde{\bm{X}}_i$, где $\widetilde{\bm{X}}_i$ --- траекторная матрица компоненты, состоящей из одной или нескольких элементарных компонент, $r$ --- число компонент.

	    \item \textbf{Диагонализация}: каждая траекторная матрица компоненты $\widetilde{\bm{X}}_i$ переводится во временной ряд $\widetilde{\mathbb{X}}_i$с помощью диагонального усреднения.
	\end{enumerate}
	Более подробно алгоритм изложен в пособии \cite{Golyandina04}.
\end{algorithm}

\section{Разделимость}

\begin{definition}
\label{def1}
        Пусть имеется ряд $\mathbb{X} = \mathbb{X}_1 + \mathbb{X}_2$. Компоненты $\mathbb{X}_1$ и $\mathbb{X}_2$ называются слабо разделимыми, если существует такое SVD, что на этапе \ref{it:3}  алгоритма \ref{alg1} можно произвести группировку таким образом, что траекторную матрицу $\mathbf{X}$ можно представить в виде $\mathbf{X} = \mathbf{X}_1 + \mathbf{X}_2$.
\end{definition}

\begin{definition}
    Если в определении \ref{def1} представление $\mathbf{X} = \mathbf{X}_1 + \mathbf{X}_2$ можно получить при любом SVD, то компоненты $\mathbb{X}_1$ и $\mathbb{X}_2$ называются сильно разделимыми.
\end{definition}

\begin{remark}
    Если рассматривать задачу выделения компонент, то с помощью алгоритма \ref{alg1} можно получить нужное разложение только в том случае, когда компоненты сильно разделимы.
\end{remark}

\section{Oblique SSA}

\begin{definition}
    Пусть $X, Y \in \mathbb{R}^n, \mathbf{A} \in \mathbb{R}^{n \times n}$. Векторы $X, Y$ называются $\mathbf{A}$-ортогональными, если
    \begin{equation*}
        (\mathbf{A}X, Y) =  \langle X, Y\rangle _{\mathbf{A}}.
    \end{equation*}
\end{definition}
\begin{definition}
    Будем говорить, что пара матриц $(\mathbf{L}, \mathbf{R})$ согласованы с матрицей $\mathbf{X}$, если пространство столбцов $\mathbf{L}$ содержит пространство столбцов $\mathbf{X}$ и пространство столбцов $\mathbf{R}$ содержит пространство строк $\mathbf{X}$.
\end{definition}
\begin{definition}
    Пусть пара матриц $(\mathbf{L}, \mathbf{R})$ согласованна с $\mathbf{X}$. Тогда минимальное разложение ранга $r$
    \begin{equation*}
        \mathbf{X} = \sum_{i = 1}^{r}\sigma_iP_iQ_i^{\mathrm{T}}
    \end{equation*}\\
    будем называть $(\mathbf{L}, \mathbf{R})$-SVD ( $(\mathbf{L}, \mathbf{R})$-сингулярным разложением) матрицы $\mathbf{X}$.
\end{definition}

Алгоритм построения $(\mathbf{L}, \mathbf{R})$-SVD подробно описан в разделе 3 статьи \cite{Golyandina15}.

\begin{definition}
    Пусть длина окна $L$ фиксирована. Ряды $\mathbb{X}^{(1)}$ и $\mathbb{X}^{(2)}$ называются слабо $(\mathbf{L}, \mathbf{R})$-разделимыми, если их траекторные пространства столбцов $\mathbf{L}$-ортогональны и траекторные пространства строк $\mathbf{R}$-ортогональны между собой, т.е.
    \begin{equation*}
        (\mathbf{X}^{(1)})^{\mathrm{T}}\mathbf{LX}^{(2)} = 0_{K \times K}, (\mathbf{X}^{(1)})\mathbf{R}(\mathbf{X}^{(2)})^{\mathrm{T}} = 0_{L \times L}.
    \end{equation*}

\end{definition}

\begin{definition}
    Ряды $\mathbb{X}^{(1)}$ и $\mathbb{X}^{(2)}$ называются сильно $(\mathbf{L}, \mathbf{R})$-разделимыми, если они слабо $(\mathbf{L}, \mathbf{R})$-разделимы и при этом наборы сингулярных чисел из SVD траекторных матриц $\mathbf{X}^{(1)}$ и $\mathbf{X}^{(2)}$ не пересекаются.
\end{definition}


\subsection{Nested Oblique SSA}
Будем рассматривать ряды конечной размерности, описанные в статье \cite{Shlemov}.
    \begin{definition}
        Ряд $\mathbb{S} = (s_1, .., s_N)$ управлется линейной рекуррентной формулой (ЛРФ), если
        \begin{equation*}
            s_{r+i} = a_1s_{r+i - 1} + a_2s_{r+i - 2} + . . . + a_rs_i
, \forall i \in 1, . . . , N - r.
        \end{equation*}
        При этом $r$ называется порядком ЛРФ.
    \end{definition}
    Для любого ряда, управляемого ЛРФ существует единственная ЛРФ минимального порядка.
    При этом элементы ряда $\mathbb{S}$ могут быть представлены в виде:
    \begin{equation*}
        s_j =\sum^n_{k=1} P_k(j)\mu^j_k
,
    \end{equation*}
где $P_k(j) \in \mathbb{C}[j]$ - полином степени не более чем $m_k - 1$, $m_k$ - кратность корня, причем $\sum_{k = 1}^nm_k = r$.
    Таким образом, ряд конечной размерности представим в виде
суммы экспонент, умноженных на многочлен.

\begin{remark}
 Для любого ряда конечного ранга $\mathbb{X} = \mathbb{X}_1 + \mathbb{X}_2$ существуют такие матрицы $\mathbf{L}, \mathbf{R}$, что ряды $\mathbb{X}_1, \mathbb{X}_2$ ($\mathbf{L}, \mathbf{R}$)-разделимы.
\end{remark}

Это утверждение влечет за собой следующий алгоритм:

\begin{algorithm}
    В условиях раздела \ref{sec12}:
    Входные данные: траекторная матрица $\mathbf{X}$, соответствующая ряду $\mathbb{X}$ конечного ранга $r$, согласованные с ней $\mathbf{L}, \mathbf{R}$.\\
    Результат: разложение ряда $\mathbb{X} = \sum_{i = 1}^{r}\mathbb{X}_i$.
\end{algorithm}
    \begin{enumerate}
        \item Строится $(\mathbf{L}, \mathbf{R})$-SVD матрицы $\mathbf{X}$.
        \item Группируются траекторные матрицы элементарных компонент.
        \item Производится диагональное усреднение траекторных матриц.
    \end{enumerate}
    Таким образом, результат алгоритма --- разложение временного ряда, соответствующего траекторной матрице $\mathbf{X}$.
    \begin{remark}
    Nested Oblique SSA не может быть применен для реальных задач, так как матрицы $\mathbf{L}, \mathbf{R}$ определяются траекторными пространствами компонент, которые мы хотим получить в разложении, при этом эти пространства неизвестны, так как мы не обладаем знаниями о компонентах.
    \end{remark}
    Существуют алгоритмы, в которых матрицы $\mathbf{L}, \mathbf{R}$ некоторым образом находятся. Рассмотрим алгоритм EOSSA, обобщенный на случай кратных сигнальных корней (случай с простыми корнями подробно изложен в работе \cite{Shlemov}).
    
\subsection{ESPRIT-motivated Oblique SSA, обобщенный случай}

Метод EOSSA (ESPRIT-based Oblique SSA) использует метод ESPRIT, описанный в статье \cite{Roy89}, чтобы разделять компоненты в случае недостатка разделимости компонент.

Для составления алгоритма понадобится следущие определение и теорема:

\begin{definition}
        Функция
        \begin{equation*}
            F_m[X] = \begin{cases}
            0, & m < 0, \\
            1, & m = 0, \\
            \frac{1}{m!}\prod_{i=0}^{m - 1}(X - i), & m>0,
            \end{cases}
        \end{equation*}
        называется убывающим факториалом.
    \end{definition}

\begin{theorem}
\label{th4}
Пусть ряд $\mathbb{S}$ длины $N$ --— ряд конечной размерности r, n --- число различных сигнальных корней характеристического полинома минимальной управляющей ЛРФ, $L > r$, $K = N - L + 1 > r$, $\mathbf{S} = \mathcal{T}_L\mathbb{S}$ --— $L$-траекторная матрица. Пусть $\mathbf{S} = \mathbf{P\Sigma Q}^{\mathrm{T}}$ --— сингулярное разложение матрицы $\mathbf{S}$, т.е. $\mathbf{P} \in \mathbb{R}^{L \times r}$, $\mathbf{Q} \in \mathbb{R}^{K \times r}$ и $\mathbf{\Sigma} \in \mathbb{R}^{r \times r}$. \\
    \hspace*{0.5cm} Тогда:
    \begin{enumerate}
        \item Матрица $\mathbf{P}$ обладает сдвиговым свойством: $\overline{\mathbf{P}} = \underline{\mathbf{P}}\mathbf{M}$ для некоторой матрицы $\mathbf{M} \in \mathbb{R}^{r \times r}$.
        \item Собственные числа матрицы $\mathbf{M}$ и их кратности совпадают с корнями х.п. $\mu_k$ и их кратностями.
        \item Пусть $\mathbf{M} = \mathbf{TJ}\mathbf{T}^{-1}, \mathbf{T} \in \mathbb{C}^{r \times r}$ --- каноническое жорданово представление матрицы $\mathbf{M}$:
        \begin{equation*}
            \mathbf{J} = \begin{pmatrix}
                    \mathbf{J}_1 & 0 & \ldots & 0\\
                    0 & \mathbf{J}_2 & \ddots & \vdots \\
                    \vdots & \ddots & \ddots & 0 \\
                    0 & \ldots & 0 & \mathbf{J}_n \\
                    \end{pmatrix},
            \mathbf{J_k} =         \begin{pmatrix}
                    \mu_k & 1 & 0 & \ldots & 0 \\
                    0 & \mu_k & 1 & \ddots & \vdots \\
                    0 & 0 & \mu_k & \ddots & 0 \\
                    \ldots & \ddots & \ddots & \ddots & 1 \\
                    0 & \ldots & 0 & 0 & \mu_k \\
                    \end{pmatrix}
        \end{equation*}
        Тогда $\mathbf{S} = \mathbf{\Pi H\Psi}^{\mathrm{T}} = (\mathbf{PT})(\mathbf{T}^{-1}\mathbf{\Sigma Q}^{\mathrm{T}}), \mathbf{\Pi} \in \mathbb{C}^{L \times r}, \mathbf{\Psi} \in \mathbb{C}^{K \times r}, \mathbf{H} \in \mathbb{C}^{r \times r}$, где $\mathbf{\Pi} = \mathbf{PT}$ и $\mathbf{H\Psi}^{\mathrm{T}} = \mathbf{T}^{-1}\mathbf{\Sigma Q}^{\mathrm{T}}$, причем:
        \begin{itemize}
            \item $\mathbf{\Pi} = [\Pi_1: \ldots :\Pi_r], \mathbf{\Pi}_k \in \mathbb{C}^{L \times M_k}$ соответствует k-ому сигнальному корню кратности $M_k$; $\mathbf{\Pi}_k[i, j] = F_j[i]\mu_k^{i - j}$, где $i = 0..L-1, j = 0..M_k-1$.
            \item Матрица $\mathbf{H}$ имеет блочно-диагональный вид:
            \begin{equation*}
                \mathbf{H} = \begin{pmatrix}
                    \mathbf{H}_1 & 0 & \ldots & 0\\
                    0 & \mathbf{H}_2 & \ddots & \vdots \\
                    \vdots & \ddots & \ddots & 0 \\
                    0 & \ldots & 0 & \mathbf{H}_n \\
                    \end{pmatrix},
            \end{equation*}
            причем $\mathbf{H_k} \in \mathbb{C}^{M_k \times M_k}$, и $\mathbf{H}_k$ верхне анти-треугольные, ганкелевы:
            \begin{equation*}
                \mathbf{H}_k = \begin{pmatrix}
                    \beta _{(k, 0)} & \beta _{(k, 1)} & \ldots & \beta _{(k, M_k - 1)}\\
                    \beta _{(k, 1)} & \iddots & \iddots & \vdots \\
                    \vdots & \iddots & \iddots & 0 \\
                    \beta _{(k, M_k - 1)} & \ldots & 0 & 0 \\
                    \end{pmatrix},
            \end{equation*}
            $\beta_{(k, p)} = \mu^{p + 1} \sum_{\alpha = 0}^{M_k - 1}K_{(p, \alpha)}c_{\alpha}$, $c_{\alpha} \in \mathbb{C},$
            \begin{equation*}
                K_{(i, j)} = \begin{cases}
                1, & i = 0 \\
                0, & i > j \\
                (i + 1)^j - \sum_{k = 0}^{M_k - 1}K_{(k, j)}c_{\alpha}, & иначе
                \end{cases}
            \end{equation*}

            \item $\mathbf{\Psi} = [\Psi_1: \ldots :\Psi_r], \mathbf{\Psi}_k \in \mathbb{C}^{K \times M_k}$ соответствует k-ому сигнальному корню кратности $M_k$; $\mathbf{\Psi}_k[i, j] = F_j[i]\mu_k^{i - j}$, где $i = 0..K-1, j = 0..M_k-1$.
        \end{itemize}
    \end{enumerate}

\end{theorem}

Доказательство этой теоремы было приведено в прошлой работе, также в этой работе был составлен следующий алгоритм.

\begin{algorithm}
Входные данные: траекторная матрица сигнала $\mathbf{S}$, соответствующая ряду $\mathbb{S}$, $n$ --- количество компонент.\\
Результат: разложение ряда $\mathbb{S} = \sum_{i = 1}^{n}\mathbb{S}_i$.\\
 В условия теоремы \ref{th4}:
\begin{enumerate}
            \item Получаем сингулярное разложение траекторной матрицы сигнала $\mathbf{S} = \mathbf{P\Sigma Q}^{\mathrm{T}}$
            \item Оцениваем сдвиговую матрицу $\mathbf{M}$.
            \item Получаем каноническое жорданово представление сдвиговой матрицы $\mathbf{M} = \mathbf{TJ}\mathbf{T}^{-1}$.
            \item Строим разложение $\mathbf{S} = (\mathbf{PT})(\mathbf{T}^{-1}\mathbf{\Sigma Q}^{\mathrm{T}}) = \mathbf{\Pi H \Psi}^{\mathrm{T}} = \sum_{k = 1}^{n}\Pi_k H_k\Psi^{\mathrm{T}}_k$.
            \item Производим группировку попарно сопряженных компонент.
            \item Производим восстановление компонент через диагональное усреднение.
        \end{enumerate}
\end{algorithm}

В этом алгоритме требуется получение канонического жорданового представления матрицы, для этого необходимо найти собственные числа матрицы, определить их кратность, построить жорданов базис. Однако сделать все это с помощью программы в общем случае возможно только приближенно, поэтому необходимо придумать алгоритм, который будет это делать.
\chapter{Обобщение алгоритма EOSSA на случай кратных корней}
\section{Получение жорданового базиса матрицы}

\subsection{Получение собственных чисел и определение их кратностей}

Собственные числа матрицы можно получить с помощью спектрального разложения. При этом в программе вычисления приближенные, поэтому кратные корни получаются простыми, которые при этом немного различаются. Чтобы определить кратность, а так же правильно найти значение корня, будем отбирать корни в кластер, если расстояние между ними меньше некоторого порогового, значением кратного корня будет среднее значение в соответствующем кластере. Составим алгоритм примитивной кластеризации.

\begin{algorithm}
Входные данные: набор корней $\{ d_i\}_{i = 1}^{r}$, параметр $\varepsilon$. Результат: набор корней $\{ b_i\}_{i = 1}^{n}$ и набор соответствующих кратностей $\{ m_i\}_{i = 1}^{n}$, причем $\sum_{i = 1}^{n}m_i = r$. \\

До тех пор, пока есть корни, не помещенные в какой-либо кластер:
\begin{enumerate}
    \item Помещаем неиспользованный корень $d_i$ в пустой кластер (в пустой набор), этот корень при этом помечается как использованный. Соответствующая кластеру кратность будет единицей. 
    \item Для всех непомеченных корней $d_j$: если $|d_i - d_j| < \varepsilon$, то $d_j$ помещаем в текущий кластер, кратность, соответствующая кластеру, увеличивается на единицу.
\end{enumerate}
Для каждого кластера вычисляется затем среднее значение корней в нем - это и будет оценкой кратного корня.

\end{algorithm}

\subsection{Получение жорданового базиса}

Так как получать жорданов базис придется приближенно, то необходимо сначала придумать как приближенно определить линейную зависимость векторов, ортогональность векторов, а также ранг матрицы.

\begin{definition}
        \textit{Приближенным рангом матрицы} относительно порога $\varepsilon$ будем называть количество сингулярных чисел в разложении этой матрицы, которые больше этого порога.
\end{definition}

\begin{definition}
        Будем говорить, что вектора $\{ v_i \}_{i = 1}^n$ \textit{линейно зависимы} относительно порога $\varepsilon$, если существует такой набор $\{ \alpha_i \}_{i = 1}^n$, $\sum_{i = 1}^n\alpha_i^2=1$, для которого $|\sum_{i = 1}^n \alpha_i v_i| < \varepsilon$. В ином случае будем называть их \textit{линейно независимыми} относительно порога $\varepsilon$.
\end{definition}

\begin{definition}
        Будем говорить, что вектора $v_1, v_2$ ортогональны относительно порога $\varepsilon$, если $|(v_1, v_2)| < \varepsilon$.
\end{definition}

Используя определения для приближенных вычислений, модифицируем алгоритм получения жорданового базиса для случая с приближенными вычислениями. Для составления этого алгоритма понадобятся вспомогательные. Следующий алгоритм находит базис векторов с помощью приближенных вычислений.

\begin{algorithm}
Входные данные: набор векторов $\{ v_i \}_{i = 1}^n$, параметр $\varepsilon$.

Результат: поднабор векторов $\{ v_{i_k}\}_{k = 1}^r$.
\\
Положим в набор любой вектор $v_i$ из набора, такой, что $||v_i|| > \varepsilon$. Далее для всех остальных векторов из входного набора:
\begin{itemize}
    \item Если $||v_j|| < \varepsilon$, то вектор игнорируется.
    \item Если $||v_j|| \geq \varepsilon$, то в случае, если $v_j$ приближенно линейно независим с векторами, ранее помещенными в набор-базис, помещаем $v_j$ в результирующий набор. В ином случае игнорируем этот вектор.
\end{itemize}
\end{algorithm}

\begin{remark}
    Для того, чтобы проверить приближенно линейную независимость, воспользуемся следующим правилом: \\
    Пусть имеется набор векторов $\{ v_i \}_{i = 1}^n$ и вектор $x$ такой, что $||x|| > \varepsilon$. Тогда вычислим аппроксимацию $x$ векторами из набора, найдем такой вектор $\alpha$, что $\mathbf{V}\alpha \approx x$. Теперь если $||\mathbf{V}\alpha - x|| > \varepsilon$, то будем считать, что $x$ приближенно линейно независим с векторами из набора. В качестве способа аппроксимации выберем QR разложение.
\end{remark}

\begin{remark}
    В случае, если $||v_i|| < \varepsilon$ для всех $i = 1, \dots, n$, то результатом будет пустой набор векторов.
\end{remark}
Для нахождения жорданового базиса приближенно необходимо дополнять одно пространство до другого, рассмотрим следующий алгоритм. Пусть даны два набора векторов $\{b^{(1)}_i\}_{i = 1}^n$ и $\{b^{(2)}_i\}_{i = 1}^m$, $m < n$. Будем дополнять первый набор векторами из пространства, порожденного векторами из второго набора, так, чтобы добавленные вектора были ортогональны векторам из первого набора. Отметим, что такая задача равносильна тому, чтобы последовательно находить вектора $x$ такие, что $\mathbf{B}_2^T\mathbf{B}_1x \approx \mathds{O}_{m}$, присоединяя $\mathbf{B}_1x$ ко второму набору, увеличивая $m$ на единицу. Матрицы $\mathbf{B_1}$ и $\mathbf{B_2}$ здесь - это матрицы, в столбцах которых находятся вектора из соответствующих наборов. Таким образом мы получаем базис ортогонального дополнения второго пространства до первого. Это необходимо для решения задачи построения жорданова базиса, так как нельзя обойтись без вычисления базиса ядра оператора. Рассмотрим теперь алгоритм.

\begin{algorithm}\label{alg6}
Входные данные: матрицы, столбцы которых содержат вектора базиса двух пространств: $\mathbf{B}_1$ и $\mathbf{B}_2$, параметр $\varepsilon$. Второе пространство при этом будет дополняться приближенно линейно независимыми векторами до первого.
\\
Результат: набор векторов $Q := \{ q_i \}_{i = 1}^{n - m}$
\begin{enumerate}
    \item Рассмотрим матрицу $\mathbf{S} := \mathbf{B}_{2}^T\mathbf{B}_{1}$. Столбцы матрицы $\mathbf{S}$, для которых выполняется $||S^{(i)}|| < \varepsilon$, помещаем в набор $Q$. \label{alg6p1}
    \item Далее пока количество векторов в наборе $Q$ меньше, чем $n_v := n - m$, выполняем следующее:
    \begin{enumerate}
        \item Вычисляется матрица $\mathbf{S} := \mathbf{B}_{2}^T\mathbf{B}_{1}$.
        \item Приближенно находится решение системы $\mathbf{S}x = \mathds{O}_m$. \label{alg6p2b}
        \item Присоединяем ко второму набору вектор $\mathbf{B}_1x$, соответственно, помещаем этот вектор в новый столбец матрицы $\mathbf{B}_1$, увеличиваем $m$ на единицу.
    \end{enumerate}
\end{enumerate}
\end{algorithm}

\begin{remark}
    В пункте $\ref{alg6p1}$ алгоритма $\ref{alg6}$ отбираются вектора из первого набора, которые приближенно ортогональны всем векторам из второго набора, соответственно, они принадлежат ортогональному дополнению.
\end{remark}

\begin{remark}
    В пункте \ref{alg6p2b} алгоритма \ref{alg6} приближенное решение будем находить с помощью QR разложения, однако для этого необходимо свести систему к системе вида $\mathbf{A}x=b$. Для этого рассмотрим два случая:
    \begin{enumerate}
        \item Приближенный ранг матрицы $\mathbf{S}$ равен единице. В этом случае найдем в первой строке элемент такой, что $|s_{1, i}| > \varepsilon$. Обозначим индекс столбца этого элемента как $n_0$. Также обозначим $I_1:=I \setminus \{ n_0 \}$ - множество индексов столбцов матрицы $\mathbf{S}$ без индекса столбца выбранного элемента. Тогда решение системы $\mathbf{S}x=\mathds{O}_m$ - это решение системы $a$
    \end{enumerate}
\end{remark}

\bibliography{refs}
\end{document}